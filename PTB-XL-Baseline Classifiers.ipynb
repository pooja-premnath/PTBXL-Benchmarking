{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d87f10e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ecg_id</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>scp_codes</th>\n",
       "      <th>filename_lr</th>\n",
       "      <th>diagnostic_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15709.0</td>\n",
       "      <td>{'NORM': 100.0, 'LVOLT': 0.0, 'SR': 0.0}</td>\n",
       "      <td>records100/00000/00001_lr</td>\n",
       "      <td>NORM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>13243.0</td>\n",
       "      <td>{'NORM': 80.0, 'SBRAD': 0.0}</td>\n",
       "      <td>records100/00000/00002_lr</td>\n",
       "      <td>NORM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>20372.0</td>\n",
       "      <td>{'NORM': 100.0, 'SR': 0.0}</td>\n",
       "      <td>records100/00000/00003_lr</td>\n",
       "      <td>NORM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>17014.0</td>\n",
       "      <td>{'NORM': 100.0, 'SR': 0.0}</td>\n",
       "      <td>records100/00000/00004_lr</td>\n",
       "      <td>NORM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>17448.0</td>\n",
       "      <td>{'NORM': 100.0, 'SR': 0.0}</td>\n",
       "      <td>records100/00000/00005_lr</td>\n",
       "      <td>NORM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ecg_id  patient_id                                 scp_codes  \\\n",
       "0       1     15709.0  {'NORM': 100.0, 'LVOLT': 0.0, 'SR': 0.0}   \n",
       "1       2     13243.0              {'NORM': 80.0, 'SBRAD': 0.0}   \n",
       "2       3     20372.0                {'NORM': 100.0, 'SR': 0.0}   \n",
       "3       4     17014.0                {'NORM': 100.0, 'SR': 0.0}   \n",
       "4       5     17448.0                {'NORM': 100.0, 'SR': 0.0}   \n",
       "\n",
       "                 filename_lr diagnostic_class  \n",
       "0  records100/00000/00001_lr             NORM  \n",
       "1  records100/00000/00002_lr             NORM  \n",
       "2  records100/00000/00003_lr             NORM  \n",
       "3  records100/00000/00004_lr             NORM  \n",
       "4  records100/00000/00005_lr             NORM  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files\n",
    "records_csv = pd.read_csv(\"/home/ppremnat/PTB-XL/ptbxl_database.csv\", usecols=['ecg_id', 'patient_id', 'scp_codes', 'filename_lr'])\n",
    "\n",
    "# Define the diagnostic classes\n",
    "diagnostics = {\n",
    "    \"NORM\": ['NORM', 'CSD'],\n",
    "    \"STTC\": ['NDT', 'NST_', 'DIG', 'LNGQT', 'ISC_', 'ISCAL', 'ISCIN', 'ISCIL', 'ISCAS', 'ISCLA', 'ANEUR', 'EL', 'ISCAN'],\n",
    "    \"MI\": ['IMI', 'ASMI', 'ILMI', 'AMI', 'ALMI', 'INJAS', 'LMI', 'INJAL', 'IPLMI', 'IPMI', 'INJIN', 'INJLA', 'PMI', 'INJIL'],\n",
    "    \"HYP\": ['LVH', 'LAO/LAE', 'RVH', 'RAO/RAE', 'SEHYP'],\n",
    "    \"CD\": ['LAFB', 'IRBBB', '1AVB', 'IVCD', 'CRBBB', 'CLBBB', 'LPFB', 'WPW', 'ILBBB', '3AVB', '2AVB'],\n",
    "    \"OTHER\": ['AFLT', 'AFIB', 'PSVT', 'STACH', 'PVC', 'PACE', 'PAC']\n",
    "}\n",
    "\n",
    "# Create a reverse mapping from SCP code to diagnostic class\n",
    "scp_to_class = {code: cls for cls, codes in diagnostics.items() for code in codes}\n",
    "\n",
    "# Function to classify the record\n",
    "def classify_record(scp_codes):\n",
    "    scp_dict = eval(scp_codes)\n",
    "    for scp_code in scp_dict.keys():\n",
    "        if scp_code in scp_to_class:\n",
    "            return scp_to_class[scp_code]\n",
    "    return 'OTHER'  # Default to 'OTHER' if no SCP code matches\n",
    "\n",
    "# Apply the classification function to the dataframe\n",
    "records_csv['diagnostic_class'] = records_csv['scp_codes'].apply(classify_record)\n",
    "\n",
    "records_csv.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73b48f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NORM     9514\n",
       "MI       5422\n",
       "STTC     2804\n",
       "CD       2322\n",
       "HYP      1307\n",
       "OTHER     430\n",
       "Name: diagnostic_class, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records_csv['diagnostic_class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "542c2ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "records_csv.drop(columns=['scp_codes'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e76ce594",
   "metadata": {},
   "outputs": [],
   "source": [
    "records =records_csv[records_csv['diagnostic_class'] != 'OTHER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "325912ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8432764b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ecg_id</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>filename_lr</th>\n",
       "      <th>diagnostic_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15709.0</td>\n",
       "      <td>records100/00000/00001_lr</td>\n",
       "      <td>NORM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>13243.0</td>\n",
       "      <td>records100/00000/00002_lr</td>\n",
       "      <td>NORM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>20372.0</td>\n",
       "      <td>records100/00000/00003_lr</td>\n",
       "      <td>NORM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>17014.0</td>\n",
       "      <td>records100/00000/00004_lr</td>\n",
       "      <td>NORM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>17448.0</td>\n",
       "      <td>records100/00000/00005_lr</td>\n",
       "      <td>NORM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ecg_id  patient_id                filename_lr diagnostic_class\n",
       "0       1     15709.0  records100/00000/00001_lr             NORM\n",
       "1       2     13243.0  records100/00000/00002_lr             NORM\n",
       "2       3     20372.0  records100/00000/00003_lr             NORM\n",
       "3       4     17014.0  records100/00000/00004_lr             NORM\n",
       "4       5     17448.0  records100/00000/00005_lr             NORM"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d481d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NORM    9514\n",
       "MI      5422\n",
       "STTC    2804\n",
       "CD      2322\n",
       "HYP     1307\n",
       "Name: diagnostic_class, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['diagnostic_class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c032dcdb",
   "metadata": {},
   "source": [
    "## ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a29ac9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import wfdb\n",
    "\n",
    "# Custom Dataset for ECG data\n",
    "class ECGDataset(Dataset):\n",
    "    def __init__(self, df, base_path, transform=None):\n",
    "        self.df = df\n",
    "        self.base_path = base_path\n",
    "        self.transform = transform\n",
    "        self.class_map = {\"NORM\": 0, \"MI\": 1, \"STTC\": 2, \"CD\": 3, \"HYP\": 4}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        row = self.df.iloc[idx]\n",
    "        record_path = os.path.join(self.base_path, row['filename_lr'])\n",
    "        signal, _ = wfdb.rdsamp(record_path)\n",
    "\n",
    "        # Convert signal to torch tensor\n",
    "        signal = torch.FloatTensor(signal.T)\n",
    "\n",
    "        # Encode the diagnostic class\n",
    "        label = torch.tensor(self.class_map[row['diagnostic_class']], dtype=torch.long)\n",
    "\n",
    "        if self.transform:\n",
    "            signal = self.transform(signal)\n",
    "\n",
    "        return signal, label\n",
    "\n",
    "\n",
    "# Split the DataFrame\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['diagnostic_class'])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ECGDataset(train_df, '/home/ppremnat/PTB-XL/')\n",
    "test_dataset = ECGDataset(test_df, '/home/ppremnat/PTB-XL/')\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "178eff94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResidualUnit(nn.Module):\n",
    "    def __init__(self, n_filters_in, n_filters_out, kernel_size=17, dropout_keep_prob=0.5, activation_function='relu'):\n",
    "        super(ResidualUnit, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(n_filters_in, n_filters_out, kernel_size=kernel_size, padding=(kernel_size // 2))\n",
    "        self.bn1 = nn.BatchNorm1d(n_filters_out)\n",
    "        self.activation = nn.ReLU() if activation_function == 'relu' else nn.ELU()\n",
    "        self.dropout = nn.Dropout(dropout_keep_prob)\n",
    "        self.conv2 = nn.Conv1d(n_filters_out, n_filters_out, kernel_size=kernel_size, padding=(kernel_size // 2))\n",
    "        self.bn2 = nn.BatchNorm1d(n_filters_out)\n",
    "        self.shortcut = nn.Conv1d(n_filters_in, n_filters_out, kernel_size=1) if n_filters_in != n_filters_out else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(f\"Input x: {x.shape}\")\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        #print(f\"After conv1: {out.shape}\")\n",
    "        out = self.bn1(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        #print(f\"After conv2: {out.shape}\")\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.shortcut is not None:\n",
    "            identity = self.shortcut(identity)\n",
    "            #print(f\"After shortcut: {identity.shape}\")\n",
    "\n",
    "        out += identity\n",
    "        out = self.activation(out)\n",
    "        #print(f\"Output out: {out.shape}\")\n",
    "\n",
    "        return out\n",
    "\n",
    "class ECG_ResNet(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super(ECG_ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(12, 64, kernel_size=17, padding=8)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.res1 = ResidualUnit(64, 128)\n",
    "        self.res2 = ResidualUnit(128, 196)\n",
    "        self.res3 = ResidualUnit(196, 256)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(f\"Input x: {x.shape}\")\n",
    "        x = self.conv1(x)\n",
    "        #print(f\"After conv1: {x.shape}\")\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.res1(x)\n",
    "        #print(f\"After res1: {x.shape}\")\n",
    "        x = self.res2(x)\n",
    "        #print(f\"After res2: {x.shape}\")\n",
    "        x = self.res3(x)\n",
    "        #print(f\"After res3: {x.shape}\")\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        #print(f\"After avgpool: {x.shape}\")\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        #print(f\"Output x: {x.shape}\")\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "157d4559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [1/535], Loss: 1.6643\n",
      "Epoch [1/10], Step [101/535], Loss: 1.0355\n",
      "Epoch [1/10], Step [201/535], Loss: 0.9031\n",
      "Epoch [1/10], Step [301/535], Loss: 0.9596\n",
      "Epoch [1/10], Step [401/535], Loss: 0.5636\n",
      "Epoch [1/10], Step [501/535], Loss: 0.7256\n",
      "Epoch [1/10] Training Loss: 0.9390\n",
      "Epoch [1/10] Training Precision: 0.6270\n",
      "Epoch [1/10] Training Recall: 0.6491\n",
      "Epoch [1/10] Training F1-Score: 0.6242\n",
      "\n",
      "\n",
      "Epoch [1/10] Test Loss: 0.8758\n",
      "Epoch [1/10] Test Accuracy: 67.59%\n",
      "Epoch [1/10] Test Precision: 0.6778\n",
      "Epoch [1/10] Test Recall: 0.6759\n",
      "Epoch [1/10] Test F1-Score: 0.6528\n",
      "Epoch [2/10], Step [1/535], Loss: 0.8676\n",
      "Epoch [2/10], Step [101/535], Loss: 0.7104\n",
      "Epoch [2/10], Step [201/535], Loss: 0.6765\n",
      "Epoch [2/10], Step [301/535], Loss: 0.7162\n",
      "Epoch [2/10], Step [401/535], Loss: 0.5679\n",
      "Epoch [2/10], Step [501/535], Loss: 0.7796\n",
      "Epoch [2/10] Training Loss: 0.7992\n",
      "Epoch [2/10] Training Precision: 0.6915\n",
      "Epoch [2/10] Training Recall: 0.7036\n",
      "Epoch [2/10] Training F1-Score: 0.6923\n",
      "\n",
      "\n",
      "Epoch [2/10] Test Loss: 0.7802\n",
      "Epoch [2/10] Test Accuracy: 71.01%\n",
      "Epoch [2/10] Test Precision: 0.7065\n",
      "Epoch [2/10] Test Recall: 0.7101\n",
      "Epoch [2/10] Test F1-Score: 0.6935\n",
      "Epoch [3/10], Step [1/535], Loss: 0.8599\n",
      "Epoch [3/10], Step [101/535], Loss: 0.7516\n",
      "Epoch [3/10], Step [201/535], Loss: 0.9551\n",
      "Epoch [3/10], Step [301/535], Loss: 0.5199\n",
      "Epoch [3/10], Step [401/535], Loss: 0.6784\n",
      "Epoch [3/10], Step [501/535], Loss: 0.8003\n",
      "Epoch [3/10] Training Loss: 0.7378\n",
      "Epoch [3/10] Training Precision: 0.7162\n",
      "Epoch [3/10] Training Recall: 0.7264\n",
      "Epoch [3/10] Training F1-Score: 0.7176\n",
      "\n",
      "\n",
      "Epoch [3/10] Test Loss: 0.7262\n",
      "Epoch [3/10] Test Accuracy: 72.72%\n",
      "Epoch [3/10] Test Precision: 0.7188\n",
      "Epoch [3/10] Test Recall: 0.7272\n",
      "Epoch [3/10] Test F1-Score: 0.7177\n",
      "Epoch [4/10], Step [1/535], Loss: 0.8152\n",
      "Epoch [4/10], Step [101/535], Loss: 0.6544\n",
      "Epoch [4/10], Step [201/535], Loss: 0.5519\n",
      "Epoch [4/10], Step [301/535], Loss: 1.0209\n",
      "Epoch [4/10], Step [401/535], Loss: 0.8252\n",
      "Epoch [4/10], Step [501/535], Loss: 0.5737\n",
      "Epoch [4/10] Training Loss: 0.6987\n",
      "Epoch [4/10] Training Precision: 0.7341\n",
      "Epoch [4/10] Training Recall: 0.7434\n",
      "Epoch [4/10] Training F1-Score: 0.7354\n",
      "\n",
      "\n",
      "Epoch [4/10] Test Loss: 0.6975\n",
      "Epoch [4/10] Test Accuracy: 73.47%\n",
      "Epoch [4/10] Test Precision: 0.7244\n",
      "Epoch [4/10] Test Recall: 0.7347\n",
      "Epoch [4/10] Test F1-Score: 0.7170\n",
      "Epoch [5/10], Step [1/535], Loss: 0.6582\n",
      "Epoch [5/10], Step [101/535], Loss: 0.7773\n",
      "Epoch [5/10], Step [201/535], Loss: 0.5443\n",
      "Epoch [5/10], Step [301/535], Loss: 0.5230\n",
      "Epoch [5/10], Step [401/535], Loss: 0.8345\n",
      "Epoch [5/10], Step [501/535], Loss: 0.7590\n",
      "Epoch [5/10] Training Loss: 0.6690\n",
      "Epoch [5/10] Training Precision: 0.7430\n",
      "Epoch [5/10] Training Recall: 0.7516\n",
      "Epoch [5/10] Training F1-Score: 0.7444\n",
      "\n",
      "\n",
      "Epoch [5/10] Test Loss: 0.7165\n",
      "Epoch [5/10] Test Accuracy: 73.68%\n",
      "Epoch [5/10] Test Precision: 0.7480\n",
      "Epoch [5/10] Test Recall: 0.7368\n",
      "Epoch [5/10] Test F1-Score: 0.7404\n",
      "Epoch [6/10], Step [1/535], Loss: 0.7339\n",
      "Epoch [6/10], Step [101/535], Loss: 0.4354\n",
      "Epoch [6/10], Step [201/535], Loss: 0.5132\n",
      "Epoch [6/10], Step [301/535], Loss: 0.7457\n",
      "Epoch [6/10], Step [401/535], Loss: 0.3155\n",
      "Epoch [6/10], Step [501/535], Loss: 0.4887\n",
      "Epoch [6/10] Training Loss: 0.6375\n",
      "Epoch [6/10] Training Precision: 0.7560\n",
      "Epoch [6/10] Training Recall: 0.7637\n",
      "Epoch [6/10] Training F1-Score: 0.7574\n",
      "\n",
      "\n",
      "Epoch [6/10] Test Loss: 0.6453\n",
      "Epoch [6/10] Test Accuracy: 75.90%\n",
      "Epoch [6/10] Test Precision: 0.7609\n",
      "Epoch [6/10] Test Recall: 0.7590\n",
      "Epoch [6/10] Test F1-Score: 0.7527\n",
      "Epoch [7/10], Step [1/535], Loss: 0.7702\n",
      "Epoch [7/10], Step [101/535], Loss: 0.5753\n",
      "Epoch [7/10], Step [201/535], Loss: 0.8740\n",
      "Epoch [7/10], Step [301/535], Loss: 0.4475\n",
      "Epoch [7/10], Step [401/535], Loss: 0.5391\n",
      "Epoch [7/10], Step [501/535], Loss: 0.4459\n",
      "Epoch [7/10] Training Loss: 0.6301\n",
      "Epoch [7/10] Training Precision: 0.7608\n",
      "Epoch [7/10] Training Recall: 0.7682\n",
      "Epoch [7/10] Training F1-Score: 0.7624\n",
      "\n",
      "\n",
      "Epoch [7/10] Test Loss: 0.6396\n",
      "Epoch [7/10] Test Accuracy: 76.23%\n",
      "Epoch [7/10] Test Precision: 0.7583\n",
      "Epoch [7/10] Test Recall: 0.7623\n",
      "Epoch [7/10] Test F1-Score: 0.7561\n",
      "Epoch [8/10], Step [1/535], Loss: 0.5134\n",
      "Epoch [8/10], Step [101/535], Loss: 0.5952\n",
      "Epoch [8/10], Step [201/535], Loss: 0.7470\n",
      "Epoch [8/10], Step [301/535], Loss: 0.8591\n",
      "Epoch [8/10], Step [401/535], Loss: 0.3850\n",
      "Epoch [8/10], Step [501/535], Loss: 0.5256\n",
      "Epoch [8/10] Training Loss: 0.6066\n",
      "Epoch [8/10] Training Precision: 0.7683\n",
      "Epoch [8/10] Training Recall: 0.7754\n",
      "Epoch [8/10] Training F1-Score: 0.7695\n",
      "\n",
      "\n",
      "Epoch [8/10] Test Loss: 0.6061\n",
      "Epoch [8/10] Test Accuracy: 77.02%\n",
      "Epoch [8/10] Test Precision: 0.7630\n",
      "Epoch [8/10] Test Recall: 0.7702\n",
      "Epoch [8/10] Test F1-Score: 0.7607\n",
      "Epoch [9/10], Step [1/535], Loss: 0.6129\n",
      "Epoch [9/10], Step [101/535], Loss: 0.7192\n",
      "Epoch [9/10], Step [201/535], Loss: 0.5840\n",
      "Epoch [9/10], Step [301/535], Loss: 0.4731\n",
      "Epoch [9/10], Step [401/535], Loss: 0.5314\n",
      "Epoch [9/10], Step [501/535], Loss: 0.4486\n",
      "Epoch [9/10] Training Loss: 0.5935\n",
      "Epoch [9/10] Training Precision: 0.7731\n",
      "Epoch [9/10] Training Recall: 0.7795\n",
      "Epoch [9/10] Training F1-Score: 0.7744\n",
      "\n",
      "\n",
      "Epoch [9/10] Test Loss: 0.5991\n",
      "Epoch [9/10] Test Accuracy: 77.68%\n",
      "Epoch [9/10] Test Precision: 0.7698\n",
      "Epoch [9/10] Test Recall: 0.7768\n",
      "Epoch [9/10] Test F1-Score: 0.7715\n",
      "Epoch [10/10], Step [1/535], Loss: 0.6574\n",
      "Epoch [10/10], Step [101/535], Loss: 0.6504\n",
      "Epoch [10/10], Step [201/535], Loss: 0.4138\n",
      "Epoch [10/10], Step [301/535], Loss: 0.5858\n",
      "Epoch [10/10], Step [401/535], Loss: 0.4289\n",
      "Epoch [10/10], Step [501/535], Loss: 0.5083\n",
      "Epoch [10/10] Training Loss: 0.5769\n",
      "Epoch [10/10] Training Precision: 0.7845\n",
      "Epoch [10/10] Training Recall: 0.7898\n",
      "Epoch [10/10] Training F1-Score: 0.7852\n",
      "\n",
      "\n",
      "Epoch [10/10] Test Loss: 0.6167\n",
      "Epoch [10/10] Test Accuracy: 77.02%\n",
      "Epoch [10/10] Test Precision: 0.7726\n",
      "Epoch [10/10] Test Recall: 0.7702\n",
      "Epoch [10/10] Test F1-Score: 0.7667\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = ECG_ResNet(num_classes=5).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Initialize metrics storage\n",
    "train_precisions, train_recalls, train_f1s = [], [], []\n",
    "test_precisions, test_recalls, test_f1s = [], [], []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, (signals, labels) in enumerate(train_loader):\n",
    "        signals, labels = signals.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(signals)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Collect predictions and labels for metrics\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/10], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Calculate and print training metrics\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    train_recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    train_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    train_precisions.append(train_precision)\n",
    "    train_recalls.append(train_recall)\n",
    "    train_f1s.append(train_f1)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/10] Training Loss: {train_loss:.4f}\")\n",
    "    print(f\"Epoch [{epoch+1}/10] Training Precision: {train_precision:.4f}\")\n",
    "    print(f\"Epoch [{epoch+1}/10] Training Recall: {train_recall:.4f}\")\n",
    "    print(f\"Epoch [{epoch+1}/10] Training F1-Score: {train_f1:.4f}\")\n",
    "\n",
    "    # Testing loop\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for signals, labels in test_loader:\n",
    "            signals, labels = signals.to(device), labels.to(device)\n",
    "            outputs = model(signals)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Collect predictions and labels for metrics\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum().item()\n",
    "        \n",
    "        accuracy = 100 * correct / total\n",
    "        test_loss = running_loss / len(test_loader)\n",
    "        test_precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "        test_recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "        test_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "        test_precisions.append(test_precision)\n",
    "        test_recalls.append(test_recall)\n",
    "        test_f1s.append(test_f1)\n",
    "        \n",
    "        print()\n",
    "        print()\n",
    "        print(f\"Epoch [{epoch+1}/10] Test Loss: {test_loss:.4f}\")\n",
    "        print(f\"Epoch [{epoch+1}/10] Test Accuracy: {accuracy:.2f}%\")\n",
    "        print(f\"Epoch [{epoch+1}/10] Test Precision: {test_precision:.4f}\")\n",
    "        print(f\"Epoch [{epoch+1}/10] Test Recall: {test_recall:.4f}\")\n",
    "        print(f\"Epoch [{epoch+1}/10] Test F1-Score: {test_f1:.4f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f6c598",
   "metadata": {},
   "source": [
    "## InceptionNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc362ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class InceptionBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(InceptionBlock, self).__init__()\n",
    "        self.conv1x1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, 16, kernel_size=1),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.conv1x1_3x3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, 16, kernel_size=1),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(16, 24, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(24),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.conv1x1_5x5 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, 16, kernel_size=1),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(16, 24, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(24),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.maxpool1x1 = nn.Sequential(\n",
    "            nn.MaxPool1d(kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv1d(in_channels, 24, kernel_size=1),\n",
    "            nn.BatchNorm1d(24),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        print(f\"InceptionBlock input shape: {x.shape}\")\n",
    "        \n",
    "        branch1 = self.conv1x1(x)\n",
    "        print(f\"Branch1 (1x1) shape: {branch1.shape}\")\n",
    "        \n",
    "        branch2 = self.conv1x1_3x3(x)\n",
    "        print(f\"Branch2 (1x1 + 3x3) shape: {branch2.shape}\")\n",
    "        \n",
    "        branch3 = self.conv1x1_5x5(x)\n",
    "        print(f\"Branch3 (1x1 + 5x5) shape: {branch3.shape}\")\n",
    "        \n",
    "        branch4 = self.maxpool1x1(x)\n",
    "        print(f\"Branch4 (maxpool + 1x1) shape: {branch4.shape}\")\n",
    "        \n",
    "        outputs = torch.cat([branch1, branch2, branch3, branch4], 1)\n",
    "        print(f\"InceptionBlock output shape: {outputs.shape}\")\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "class InceptionNet(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super(InceptionNet, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(12, 64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.inception1 = InceptionBlock(64)\n",
    "        self.inception2 = InceptionBlock(64 + 16 + 24 + 24)  # Output channels from inception1\n",
    "        \n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(64 + 16 + 24 + 24, num_classes)  # Output channels after concatenation\n",
    "    \n",
    "    def forward(self, x):\n",
    "        print(f\"InceptionNet input shape: {x.shape}\")\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        print(f\"After conv1: {x.shape}\")\n",
    "        \n",
    "        x = self.inception1(x)\n",
    "        print(f\"After inception1: {x.shape}\")\n",
    "        \n",
    "        x = self.inception2(x)\n",
    "        print(f\"After inception2: {x.shape}\")\n",
    "        \n",
    "        x = self.pool(x)\n",
    "        print(f\"After pool: {x.shape}\")\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        print(f\"After view (flatten): {x.shape}\")\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        print(f\"Output shape: {x.shape}\")\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7303e576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InceptionNet input shape: torch.Size([32, 12, 1000])\n",
      "After conv1: torch.Size([32, 64, 500])\n",
      "InceptionBlock input shape: torch.Size([32, 64, 500])\n",
      "Branch1 (1x1) shape: torch.Size([32, 16, 500])\n",
      "Branch2 (1x1 + 3x3) shape: torch.Size([32, 24, 500])\n",
      "Branch3 (1x1 + 5x5) shape: torch.Size([32, 24, 500])\n",
      "Branch4 (maxpool + 1x1) shape: torch.Size([32, 24, 500])\n",
      "InceptionBlock output shape: torch.Size([32, 88, 500])\n",
      "After inception1: torch.Size([32, 88, 500])\n",
      "InceptionBlock input shape: torch.Size([32, 88, 500])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [16, 128, 1], expected input[32, 88, 500] to have 128 channels, but got 88 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 64>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m signals, labels \u001b[38;5;241m=\u001b[39m signals\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     72\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 73\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msignals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     75\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36mInceptionNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     79\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minception1(x)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAfter inception1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 82\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minception2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAfter inception2: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36mInceptionBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInceptionBlock input shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m     branch1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1x1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBranch1 (1x1) shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbranch1\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m     branch2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1x1_3x3(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py:310\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py:306\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    304\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    305\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [16, 128, 1], expected input[32, 88, 500] to have 128 channels, but got 88 channels instead"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import wfdb\n",
    "import os\n",
    "\n",
    "# Custom Dataset for ECG data\n",
    "class ECGDataset(Dataset):\n",
    "    def __init__(self, df, base_path, transform=None):\n",
    "        self.df = df\n",
    "        self.base_path = base_path\n",
    "        self.transform = transform\n",
    "        self.class_map = {\"NORM\": 0, \"MI\": 1, \"STTC\": 2, \"CD\": 3, \"HYP\": 4}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        row = self.df.iloc[idx]\n",
    "        record_path = os.path.join(self.base_path, row['filename_lr'])\n",
    "        signal, _ = wfdb.rdsamp(record_path)\n",
    "\n",
    "        # Convert signal to torch tensor\n",
    "        signal = torch.FloatTensor(signal.T)\n",
    "\n",
    "        # Encode the diagnostic class\n",
    "        label = torch.tensor(self.class_map[row['diagnostic_class']], dtype=torch.long)\n",
    "\n",
    "        if self.transform:\n",
    "            signal = self.transform(signal)\n",
    "\n",
    "        return signal, label\n",
    "\n",
    "\n",
    "# Split the DataFrame\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['diagnostic_class'])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ECGDataset(train_df, '/home/ppremnat/PTB-XL/')\n",
    "test_dataset = ECGDataset(test_df, '/home/ppremnat/PTB-XL/')\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = InceptionNet(num_classes=5).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Initialize metrics storage\n",
    "train_precisions, train_recalls, train_f1s = [], [], []\n",
    "test_precisions, test_recalls, test_f1s = [], [], []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, (signals, labels) in enumerate(train_loader):\n",
    "        signals, labels = signals.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(signals)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Collect predictions and labels for metrics\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/10], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Calculate and print training metrics\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    train_recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    train_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    train_precisions.append(train_precision)\n",
    "    train_recalls.append(train_recall)\n",
    "    train_f1s.append(train_f1)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/10] Training Loss: {train_loss:.4f}\")\n",
    "    print(f\"Epoch [{epoch+1}/10] Training Precision: {train_precision:.4f}\")\n",
    "    print(f\"Epoch [{epoch+1}/10] Training Recall: {train_recall:.4f}\")\n",
    "    print(f\"Epoch [{epoch+1}/10] Training F1-Score: {train_f1:.4f}\")\n",
    "\n",
    "    # Testing loop\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for signals, labels in test_loader:\n",
    "            signals, labels = signals.to(device), labels.to(device)\n",
    "            outputs = model(signals)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Collect predictions and labels for metrics\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum().item()\n",
    "        \n",
    "        accuracy = 100 * correct / total\n",
    "        test_loss = running_loss / len(test_loader)\n",
    "        test_precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "        test_recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "        test_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "        test_precisions.append(test_precision)\n",
    "        test_recalls.append(test_recall)\n",
    "        test_f1s.append(test_f1)\n",
    "        \n",
    "        print()\n",
    "        print()\n",
    "        print(f\"Epoch [{epoch+1}/10] Test Loss: {test_loss:.4f}\")\n",
    "        print(f\"Epoch [{epoch+1}/10] Test Accuracy: {accuracy:.2f}%\")\n",
    "        print(f\"Epoch [{epoch+1}/10] Test Precision: {test_precision:.4f}\")\n",
    "        print(f\"Epoch [{epoch+1}/10] Test Recall: {test_recall:.4f}\")\n",
    "        print(f\"Epoch [{epoch+1}/10] Test F1-Score: {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcb9404",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a8ee55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
